#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡è¯„ä¼°å’Œé«˜è´¨é‡Universeç”Ÿæˆå·¥å…·
ç­›é€‰æ•°æ®ç›¸å¯¹è¶³å¤Ÿçš„æ ‡çš„è¿›è¡Œå›æµ‹
"""

import pandas as pd
import numpy as np
from pathlib import Path
import glob
import yaml
from datetime import datetime, timedelta

class DataQualityFilter:
    def __init__(self):
        self.base_path = Path(__file__).parent.parent
        self.config = self._load_config()
        
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = self.base_path / "config/factors.yml"
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def evaluate_data_quality(self, min_coverage_rate=0.7, min_history_days=100, 
                            target_date_range=None):
        """è¯„ä¼°æ ‡çš„æ•°æ®è´¨é‡
        
        Args:
            min_coverage_rate: æœ€å°è¦†ç›–ç‡è¦æ±‚ (0-1)
            min_history_days: æœ€å°å†å²å¤©æ•°è¦æ±‚
            target_date_range: ç›®æ ‡æ—¥æœŸèŒƒå›´ (start_date, end_date)
        
        Returns:
            qualified_assets: åˆæ ¼æ ‡çš„åˆ—è¡¨åŠå…¶è´¨é‡è¯„åˆ†
        """
        print("ğŸ” å¼€å§‹æ•°æ®è´¨é‡è¯„ä¼°...")
        
        # è®¾ç½®ç›®æ ‡æ—¥æœŸèŒƒå›´
        if target_date_range is None:
            start_date = pd.to_datetime(self.config['data']['start_date'])
            end_date = pd.to_datetime(self.config['data']['end_date'])
        else:
            start_date, end_date = target_date_range
        
        target_trading_days = pd.bdate_range(start_date, end_date)
        target_days_count = len(target_trading_days)
        
        print(f"ç›®æ ‡æ—¶é—´èŒƒå›´: {start_date.date()} è‡³ {end_date.date()}")
        print(f"ç›®æ ‡äº¤æ˜“æ—¥æ•°: {target_days_count}")
        
        # åŠ è½½universe
        universe_file = self.base_path / self.config['data']['universe_file']
        universe_df = pd.read_csv(universe_file)
        universe_codes = set(universe_df['ts_code'].tolist())
        
        # æ‰«ææ•°æ®æ–‡ä»¶
        processed_path = self.base_path / self.config['paths']['processed_data']
        parquet_files = glob.glob(str(processed_path / "**/*.parquet"), recursive=True)
        
        asset_quality_scores = {}
        detailed_stats = {}
        
        print(f"\nğŸ“Š è¯„ä¼° {len(universe_codes)} ä¸ªæ ‡çš„çš„æ•°æ®è´¨é‡...")
        
        processed_count = 0
        for file_path in parquet_files:
            file_name = Path(file_path).stem
            code = file_name
            
            if code not in universe_codes:
                continue
                
            try:
                df = pd.read_parquet(file_path)
                
                # ç»Ÿä¸€å¤„ç†æ—¥æœŸç´¢å¼•
                if df.index.name == 'trade_date':
                    df.index = pd.to_datetime(df.index)
                elif 'trade_date' in df.columns:
                    df['trade_date'] = pd.to_datetime(df['trade_date'])
                    df = df.set_index('trade_date')
                
                # ç­›é€‰ç›®æ ‡æ—¥æœŸèŒƒå›´
                df_filtered = df[(df.index >= start_date) & (df.index <= end_date)]
                
                if df_filtered.empty:
                    continue
                
                # æ•°æ®è´¨é‡è¯„ä¼°æŒ‡æ ‡
                quality_metrics = self._calculate_quality_metrics(
                    df_filtered, target_trading_days, target_days_count
                )
                
                # è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ†
                quality_score = self._calculate_quality_score(quality_metrics)
                
                # åˆ¤æ–­æ˜¯å¦æ»¡è¶³æœ€ä½è¦æ±‚
                meets_requirements = (
                    quality_metrics['coverage_rate'] >= min_coverage_rate and
                    quality_metrics['available_days'] >= min_history_days and
                    quality_metrics['close_completeness'] >= 0.8  # æ”¶ç›˜ä»·å®Œæ•´æ€§è¦æ±‚
                )
                
                asset_quality_scores[code] = {
                    'quality_score': quality_score,
                    'meets_requirements': meets_requirements,
                    **quality_metrics
                }
                
                detailed_stats[code] = quality_metrics
                processed_count += 1
                
                if processed_count % 50 == 0:
                    print(f"   å·²å¤„ç†: {processed_count}/{len(universe_codes)}")
                
            except Exception as e:
                print(f"   âš ï¸  å¤„ç† {code} æ—¶å‡ºé”™: {e}")
                continue
        
        return asset_quality_scores, detailed_stats
    
    def _calculate_quality_metrics(self, df, target_trading_days, target_days_count):
        """è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡"""
        
        # åŸºç¡€æŒ‡æ ‡
        available_days = len(df)
        coverage_rate = available_days / target_days_count if target_days_count > 0 else 0
        
        # å„å­—æ®µå®Œæ•´æ€§
        close_completeness = df['close'].notna().sum() / len(df) if len(df) > 0 else 0
        vol_completeness = df['vol'].notna().sum() / len(df) if 'vol' in df.columns and len(df) > 0 else 0
        amount_completeness = df['amount'].notna().sum() / len(df) if 'amount' in df.columns and len(df) > 0 else 0
        
        # è¿ç»­æ€§è¯„ä¼°
        df_sorted = df.sort_index()
        date_gaps = []
        if len(df_sorted) > 1:
            for i in range(1, len(df_sorted)):
                gap_days = (df_sorted.index[i] - df_sorted.index[i-1]).days
                if gap_days > 3:  # è¶…è¿‡3å¤©è®¤ä¸ºæ˜¯æ•°æ®ç¼ºå£
                    date_gaps.append(gap_days)
        
        max_gap = max(date_gaps) if date_gaps else 0
        gap_count = len(date_gaps)
        
        # æ•°æ®æ›´æ–°åº¦
        last_date = df.index.max()
        days_since_last_update = (pd.Timestamp.now() - last_date).days
        
        return {
            'available_days': available_days,
            'coverage_rate': coverage_rate,
            'close_completeness': close_completeness,
            'vol_completeness': vol_completeness,
            'amount_completeness': amount_completeness,
            'max_gap_days': max_gap,
            'gap_count': gap_count,
            'days_since_last_update': days_since_last_update,
            'date_range': (df.index.min(), df.index.max())
        }
    
    def _calculate_quality_score(self, metrics):
        """è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ† (0-100)"""
        
        # å„é¡¹æŒ‡æ ‡æƒé‡
        weights = {
            'coverage_rate': 0.3,      # è¦†ç›–ç‡æƒé‡30%
            'close_completeness': 0.25, # æ”¶ç›˜ä»·å®Œæ•´æ€§25%
            'vol_completeness': 0.15,   # æˆäº¤é‡å®Œæ•´æ€§15%
            'amount_completeness': 0.15, # æˆäº¤é¢å®Œæ•´æ€§15%
            'continuity': 0.1,          # è¿ç»­æ€§10%
            'freshness': 0.05           # æ›´æ–°åº¦5%
        }
        
        # è®¡ç®—å„é¡¹å¾—åˆ†
        coverage_score = min(metrics['coverage_rate'] * 100, 100)
        close_score = metrics['close_completeness'] * 100
        vol_score = metrics['vol_completeness'] * 100
        amount_score = metrics['amount_completeness'] * 100
        
        # è¿ç»­æ€§å¾—åˆ† (åŸºäºæ•°æ®ç¼ºå£)
        if metrics['max_gap_days'] == 0:
            continuity_score = 100
        elif metrics['max_gap_days'] <= 7:
            continuity_score = 80
        elif metrics['max_gap_days'] <= 30:
            continuity_score = 60
        else:
            continuity_score = max(0, 60 - metrics['max_gap_days'])
        
        # æ›´æ–°åº¦å¾—åˆ†
        if metrics['days_since_last_update'] <= 3:
            freshness_score = 100
        elif metrics['days_since_last_update'] <= 7:
            freshness_score = 80
        elif metrics['days_since_last_update'] <= 30:
            freshness_score = 60
        else:
            freshness_score = max(0, 60 - metrics['days_since_last_update'])
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = (
            coverage_score * weights['coverage_rate'] +
            close_score * weights['close_completeness'] +
            vol_score * weights['vol_completeness'] +
            amount_score * weights['amount_completeness'] +
            continuity_score * weights['continuity'] +
            freshness_score * weights['freshness']
        )
        
        return round(total_score, 2)
    
    def generate_high_quality_universe(self, min_coverage_rate=0.7, min_history_days=100,
                                     min_quality_score=70, max_assets=None,
                                     output_file="config/universe_high_quality.csv"):
        """ç”Ÿæˆé«˜è´¨é‡æ ‡çš„æ± 
        
        Args:
            min_coverage_rate: æœ€å°è¦†ç›–ç‡
            min_history_days: æœ€å°å†å²å¤©æ•°
            min_quality_score: æœ€å°è´¨é‡è¯„åˆ†
            max_assets: æœ€å¤§æ ‡çš„æ•°é‡é™åˆ¶
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        
        print("=" * 60)
        print("ğŸ† ç”Ÿæˆé«˜è´¨é‡æ ‡çš„æ± ")
        print("=" * 60)
        
        print(f"ç­›é€‰æ¡ä»¶:")
        print(f"  æœ€å°è¦†ç›–ç‡: {min_coverage_rate*100:.1f}%")
        print(f"  æœ€å°å†å²å¤©æ•°: {min_history_days}")
        print(f"  æœ€å°è´¨é‡è¯„åˆ†: {min_quality_score}")
        if max_assets:
            print(f"  æœ€å¤§æ ‡çš„æ•°é‡: {max_assets}")
        
        # è¯„ä¼°æ•°æ®è´¨é‡
        asset_scores, detailed_stats = self.evaluate_data_quality(
            min_coverage_rate, min_history_days
        )
        
        # ç­›é€‰é«˜è´¨é‡æ ‡çš„
        qualified_assets = []
        for code, metrics in asset_scores.items():
            if (metrics['meets_requirements'] and 
                metrics['quality_score'] >= min_quality_score):
                qualified_assets.append({
                    'ts_code': code,
                    'quality_score': metrics['quality_score'],
                    'coverage_rate': metrics['coverage_rate'],
                    'available_days': metrics['available_days'],
                    'close_completeness': metrics['close_completeness'],
                    'vol_completeness': metrics['vol_completeness'],
                    'amount_completeness': metrics['amount_completeness']
                })
        
        # æŒ‰è´¨é‡è¯„åˆ†æ’åº
        qualified_assets.sort(key=lambda x: x['quality_score'], reverse=True)
        
        # é™åˆ¶æ•°é‡
        if max_assets and len(qualified_assets) > max_assets:
            qualified_assets = qualified_assets[:max_assets]
        
        print(f"\nğŸ“Š ç­›é€‰ç»“æœ:")
        print(f"  åŸå§‹æ ‡çš„æ•°: {len(asset_scores)}")
        print(f"  åˆæ ¼æ ‡çš„æ•°: {len(qualified_assets)}")
        print(f"  ç­›é€‰æˆåŠŸç‡: {len(qualified_assets)/len(asset_scores)*100:.1f}%")
        
        if qualified_assets:
            scores = [asset['quality_score'] for asset in qualified_assets]
            coverages = [asset['coverage_rate'] for asset in qualified_assets]
            
            print(f"\nğŸ“ˆ è´¨é‡ç»Ÿè®¡:")
            print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {np.mean(scores):.1f}")
            print(f"  æœ€é«˜è´¨é‡è¯„åˆ†: {np.max(scores):.1f}")
            print(f"  æœ€ä½è´¨é‡è¯„åˆ†: {np.min(scores):.1f}")
            print(f"  å¹³å‡è¦†ç›–ç‡: {np.mean(coverages)*100:.1f}%")
            
            # æ˜¾ç¤ºå‰10å
            print(f"\nğŸ… è´¨é‡å‰10å:")
            for i, asset in enumerate(qualified_assets[:10]):
                print(f"  {i+1:2d}. {asset['ts_code']}: "
                      f"è¯„åˆ†{asset['quality_score']:.1f}, "
                      f"è¦†ç›–{asset['coverage_rate']*100:.1f}%, "
                      f"{asset['available_days']}å¤©")
        
        # ä¿å­˜ç»“æœ
        if qualified_assets:
            output_path = self.base_path / output_file
            output_path.parent.mkdir(exist_ok=True)
            
            # åªä¿å­˜ts_codeåˆ—ç”¨äºuniverseé…ç½®
            universe_df = pd.DataFrame([{'ts_code': asset['ts_code']} 
                                      for asset in qualified_assets])
            universe_df.to_csv(output_path, index=False)
            
            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            detailed_output = output_path.parent / f"{output_path.stem}_detailed.csv"
            detailed_df = pd.DataFrame(qualified_assets)
            detailed_df.to_csv(detailed_output, index=False)
            
            print(f"\nğŸ’¾ æ–‡ä»¶å·²ä¿å­˜:")
            print(f"  Universeæ–‡ä»¶: {output_path}")
            print(f"  è¯¦ç»†æŠ¥å‘Š: {detailed_output}")
            
            return qualified_assets
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ ‡çš„")
            return []
    
    def analyze_coverage_improvement(self, high_quality_universe_file):
        """åˆ†æä½¿ç”¨é«˜è´¨é‡Universeåçš„è¦†ç›–ç‡æ”¹å–„"""
        
        print("=" * 60)
        print("ğŸ“ˆ è¦†ç›–ç‡æ”¹å–„åˆ†æ")
        print("=" * 60)
        
        # æ¨¡æ‹Ÿå› å­è®¡ç®—çš„æ•°æ®è¦æ±‚
        factor_requirements = {
            'mom20': {'fields': ['close'], 'min_history': 20},
            'shortrev5': {'fields': ['close'], 'min_history': 5},
            'vol20': {'fields': ['close'], 'min_history': 20},
            'macd_signal': {'fields': ['close'], 'min_history': 34},
            'turn_mean20': {'fields': ['vol', 'amount'], 'min_history': 20},
            'amihud20': {'fields': ['close', 'amount'], 'min_history': 20}
        }
        
        # åŠ è½½é«˜è´¨é‡universe
        hq_universe_path = self.base_path / high_quality_universe_file
        if not hq_universe_path.exists():
            print(f"âŒ é«˜è´¨é‡Universeæ–‡ä»¶ä¸å­˜åœ¨: {hq_universe_path}")
            return
        
        hq_universe_df = pd.read_csv(hq_universe_path)
        hq_codes = set(hq_universe_df['ts_code'].tolist())
        
        print(f"é«˜è´¨é‡Universeæ ‡çš„æ•°: {len(hq_codes)}")
        
        # è¯„ä¼°é¢„æœŸè¦†ç›–ç‡
        asset_scores, _ = self.evaluate_data_quality(min_coverage_rate=0.5)
        
        expected_coverage = {}
        for factor_name, req in factor_requirements.items():
            valid_assets = 0
            total_observations = 0
            
            for code in hq_codes:
                if code in asset_scores:
                    metrics = asset_scores[code]
                    
                    # æ£€æŸ¥å­—æ®µè¦æ±‚
                    fields_ok = True
                    if 'close' in req['fields'] and metrics['close_completeness'] < 0.8:
                        fields_ok = False
                    if 'vol' in req['fields'] and metrics['vol_completeness'] < 0.7:
                        fields_ok = False
                    if 'amount' in req['fields'] and metrics['amount_completeness'] < 0.7:
                        fields_ok = False
                    
                    # æ£€æŸ¥å†å²é•¿åº¦
                    history_ok = metrics['available_days'] >= req['min_history']
                    
                    if fields_ok and history_ok:
                        valid_assets += 1
                        # ä¼°ç®—æœ‰æ•ˆè§‚æµ‹æ•°
                        effective_days = max(0, metrics['available_days'] - req['min_history'])
                        total_observations += effective_days
            
            coverage_rate = valid_assets / len(hq_codes) * 100 if hq_codes else 0
            expected_coverage[factor_name] = {
                'valid_assets': valid_assets,
                'total_assets': len(hq_codes),
                'coverage_rate': coverage_rate,
                'estimated_observations': total_observations
            }
        
        print(f"\nğŸ“Š é¢„æœŸå› å­è¦†ç›–ç‡:")
        for factor_name, info in expected_coverage.items():
            print(f"  {factor_name:12s}: {info['coverage_rate']:5.1f}% "
                  f"({info['valid_assets']}/{info['total_assets']} æ ‡çš„, "
                  f"~{info['estimated_observations']:,} è§‚æµ‹)")
        
        # ä¼°ç®—æ€»ä½“æ”¹å–„
        avg_coverage = np.mean([info['coverage_rate'] for info in expected_coverage.values()])
        total_estimated_obs = sum([info['estimated_observations'] for info in expected_coverage.values()])
        
        print(f"\nğŸ¯ æ€»ä½“é¢„æœŸ:")
        print(f"  å¹³å‡å› å­è¦†ç›–ç‡: {avg_coverage:.1f}%")
        print(f"  æ€»è®¡é¢„æœŸè§‚æµ‹æ•°: ~{total_estimated_obs:,}")
        print(f"  ä¸å½“å‰è¦†ç›–ç‡å¯¹æ¯”: é¢„è®¡æå‡ {avg_coverage/2.5:.1f} å€")  # å½“å‰çº¦2.5%

def main():
    """ä¸»ç¨‹åºï¼šç”Ÿæˆé«˜è´¨é‡Universe"""
    filter_tool = DataQualityFilter()
    
    print("ğŸš€ æ•°æ®è´¨é‡ç­›é€‰å·¥å…·")
    print("=" * 60)
    
    # ç”Ÿæˆé«˜è´¨é‡æ ‡çš„æ± 
    qualified_assets = filter_tool.generate_high_quality_universe(
        min_coverage_rate=0.6,    # è‡³å°‘60%è¦†ç›–ç‡
        min_history_days=50,      # è‡³å°‘50å¤©å†å²
        min_quality_score=60,     # è‡³å°‘60åˆ†è´¨é‡è¯„åˆ†
        max_assets=100,           # æœ€å¤š100ä¸ªæ ‡çš„
        output_file="config/universe_high_quality.csv"
    )
    
    if qualified_assets:
        # åˆ†æé¢„æœŸæ”¹å–„æ•ˆæœ
        filter_tool.analyze_coverage_improvement("config/universe_high_quality.csv")
        
        print(f"\nğŸ‰ é«˜è´¨é‡Universeç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•: ä¿®æ”¹ config/factors.yml ä¸­çš„ universe_file ä¸º:")
        print(f"   universe_file: \"config/universe_high_quality.csv\"")

if __name__ == "__main__":
    main()
